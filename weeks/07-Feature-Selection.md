---
layout: week
title: Week 07
doodle: /doodle.png
---

# Feature Selection

## Topics

This week's assignments will guide you through the following topics:

* How to extract feature importance values from your trained model
* How to use that information to perform feature selection

## Reading

Please read the following:

* Supplemental Reading [Feature Importance and Feature Selection With XGBoost in Python](https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/)

## Replication task

* Extract and plot feature importance values for your model
* Iterate through different importance threshold values to select a less complex model that maximizes accuracy


## Tasks

Complete the following tasks:

* Read the article on feature importance and feature selection
* Apply the techniques in the article on your own model
* Complete the weekly questions below
* Work on project proposal and elevator pitch assignments


## Weekly Questions

Answer the following questions

* What does feature importance tell us? Why is that useful to know?
* What's the difference between 'gain' and 'weight' when it comes to feature importance for tree-based models?
* Why can reducing the total number of features in our model be helpful or preferable?
